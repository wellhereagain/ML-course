{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
    "\n",
    "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/svetlanamaslennikova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "\n",
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пропуски в train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропусков в столбце keyword: 44\n",
      "Пропусков в столбце location: 1760\n"
     ]
    }
   ],
   "source": [
    "na = train.isnull().any().values\n",
    "\n",
    "for c in train.columns[na]:\n",
    "    print('Пропусков в столбце {}: {}'.format(c, np.sum(train[c].isnull())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna('')\n",
    "train.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пропусти в test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропусков в столбце keyword: 17\n",
      "Пропусков в столбце location: 773\n"
     ]
    }
   ],
   "source": [
    "na = test.isnull().any().values\n",
    "\n",
    "for c in test.columns[na]:\n",
    "    print('Пропусков в столбце {}: {}'.format(c, np.sum(test[c].isnull())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.fillna('')\n",
    "test.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (1 балл)\n",
    "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
    "\n",
    "1. Какое распределение классов в обучающей выборке?\n",
    "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "распределение классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56.746106\n",
       "1    43.253894\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 1137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (7, 5))\n",
    "sns.countplot(x = 'target',   data = train, palette = 'husl')\n",
    "train['target'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы распределены более-менее равномерно, класс 0 занимает 56,74% обучающей выборки, класс 1 - 43,25%, сильного перекоса в какой-то из классов нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "гугл сказал что ступенчатая диаграмма распределения это гистограмма распределения так что вот "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pd.DataFrame(train[train['keyword'] != '']['keyword'].value_counts()[:10])\n",
    "mask = mask.reset_index()\n",
    "train_fr_keywords = pd.merge(train, mask, left_on = 'keyword', right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='keyword_x', ylabel='count'>"
      ]
     },
     "execution_count": 1139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (18, 5))\n",
    "sns.countplot(x = 'keyword_x', hue = 'target',  data = train_fr_keywords, palette = 'husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Забавно что со словом damage, fatalities и derail почти поровну позитивных и негативных твитов, a на wreckage все позитивные (????)\n",
    "\n",
    "по остальным понятна негативная конатация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла) \n",
    "\n",
    "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1707</td>\n",
       "      <td>bridge%20collapse  Ashes 2015: AustraliaÛªs c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>5789</td>\n",
       "      <td>hail Carol Stream, Illinois GREAT MICHIGAN TEC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>7789</td>\n",
       "      <td>police Houston  CNN: Tennessee movie theater s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>8257</td>\n",
       "      <td>rioting  Still rioting in a couple of hours le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>10656</td>\n",
       "      <td>wounds Lake Highlands Crack in the path where ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>7470</td>\n",
       "      <td>obliteration Merica! @Eganator2000 There aren'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>7691</td>\n",
       "      <td>panic  just had a panic attack bc I don't have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1242</td>\n",
       "      <td>blood  Omron HEM-712C Automatic Blood Pressure...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>10862</td>\n",
       "      <td>Officials say a quarantine is in place at an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>10409</td>\n",
       "      <td>whirlwind Stamford &amp; Cork (&amp; Shropshire) I mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5329 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  target\n",
       "1186   1707  bridge%20collapse  Ashes 2015: AustraliaÛªs c...       0\n",
       "4071   5789  hail Carol Stream, Illinois GREAT MICHIGAN TEC...       1\n",
       "5461   7789  police Houston  CNN: Tennessee movie theater s...       1\n",
       "5787   8257  rioting  Still rioting in a couple of hours le...       1\n",
       "7445  10656  wounds Lake Highlands Crack in the path where ...       0\n",
       "...     ...                                                ...     ...\n",
       "5226   7470  obliteration Merica! @Eganator2000 There aren'...       0\n",
       "5390   7691  panic  just had a panic attack bc I don't have...       0\n",
       "860    1242  blood  Omron HEM-712C Automatic Blood Pressure...       0\n",
       "7603  10862    Officials say a quarantine is in place at an...       1\n",
       "7270  10409  whirlwind Stamford & Cork (& Shropshire) I mov...       1\n",
       "\n",
       "[5329 rows x 3 columns]"
      ]
     },
     "execution_count": 1140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'] = train['keyword'] + ' ' + train['location'] + ' ' + train['text']\n",
    "train = train.drop(columns=['keyword', 'location'])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction  So you have a new weapon that can...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge  The f$&amp;amp;@ing things I do for #GISHW...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police UK DT @georgegalloway: RT @Galloway4May...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock  Aftershock back to school kick off...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma Montgomery County, MD in response to tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>6115</td>\n",
       "      <td>hellfire 570 Vanderbilt; Brooklyn, NY New cock...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>4833</td>\n",
       "      <td>evacuation USA Bend Post Office roofers cut ga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>2466</td>\n",
       "      <td>collided  Monsoon flooding - Monsoon rains hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4898</th>\n",
       "      <td>6973</td>\n",
       "      <td>massacre Ireland Remember this was a massacre ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6753</th>\n",
       "      <td>9674</td>\n",
       "      <td>tornado Asheville, NC I liked a @YouTube video...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  target\n",
       "2644  3796  destruction  So you have a new weapon that can...       1\n",
       "2227  3185  deluge  The f$&amp;@ing things I do for #GISHW...       0\n",
       "5448  7769  police UK DT @georgegalloway: RT @Galloway4May...       1\n",
       "132    191  aftershock  Aftershock back to school kick off...       0\n",
       "6845  9810  trauma Montgomery County, MD in response to tr...       0\n",
       "...    ...                                                ...     ...\n",
       "4307  6115  hellfire 570 Vanderbilt; Brooklyn, NY New cock...       0\n",
       "3375  4833  evacuation USA Bend Post Office roofers cut ga...       1\n",
       "1710  2466  collided  Monsoon flooding - Monsoon rains hav...       1\n",
       "4898  6973  massacre Ireland Remember this was a massacre ...       1\n",
       "6753  9674  tornado Asheville, NC I liked a @YouTube video...       1\n",
       "\n",
       "[2284 rows x 3 columns]"
      ]
     },
     "execution_count": 1142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'] = test['keyword'] + ' ' + test['location'] + ' ' + test['text']\n",
    "test = test.drop(columns=['keyword', 'location'])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "\n",
    "Далее мы будем пока работать только с train частью.\n",
    "\n",
    "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
    "2. Какого размера получилась матрица?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 18455)"
      ]
     },
     "execution_count": 1145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5 (1 балл)\n",
    "\n",
    "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
    "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
    "\n",
    "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
    "\n",
    "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
    "\n",
    "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
    "\n",
    "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bridge': 2948,\n",
       " '20collapse': 320,\n",
       " 'ashes': 1928,\n",
       " '2015': 295,\n",
       " 'australia': 2059,\n",
       " 'ûªs': 18425,\n",
       " 'collapse': 3914,\n",
       " 'at': 1977,\n",
       " 'trent': 16483,\n",
       " 'among': 1628,\n",
       " 'worst': 17813,\n",
       " 'in': 8314,\n",
       " 'history': 7773,\n",
       " 'england': 5722,\n",
       " 'bundled': 3085,\n",
       " 'out': 11995,\n",
       " 'for': 6503,\n",
       " '60': 755,\n",
       " 'http': 7970,\n",
       " 'co': 3861,\n",
       " 't5trhjuau0': 15733,\n",
       " 'hail': 7420,\n",
       " 'carol': 3363,\n",
       " 'stream': 15419,\n",
       " 'illinois': 8249,\n",
       " 'great': 7206,\n",
       " 'michigan': 10594,\n",
       " 'technique': 15888,\n",
       " 'camp': 3261,\n",
       " 'b1g': 2147,\n",
       " 'thanks': 16013,\n",
       " 'to': 16272,\n",
       " 'bmurph1019': 2753,\n",
       " 'hail_youtsey': 7421,\n",
       " 'termn8r13': 15949,\n",
       " 'goblue': 7088,\n",
       " 'wrestleon': 17848,\n",
       " 'oaskgki6qj': 11648,\n",
       " 'police': 12634,\n",
       " 'houston': 7933,\n",
       " 'cnn': 3854,\n",
       " 'tennessee': 15932,\n",
       " 'movie': 10940,\n",
       " 'theater': 16025,\n",
       " 'shooting': 14647,\n",
       " 'suspect': 15632,\n",
       " 'killed': 9246,\n",
       " 'by': 3150,\n",
       " 'di8elzswnr': 4890,\n",
       " 'rioting': 13800,\n",
       " 'still': 15357,\n",
       " 'couple': 4204,\n",
       " 'of': 11708,\n",
       " 'hours': 7925,\n",
       " 'left': 9644,\n",
       " 'until': 16884,\n",
       " 'have': 7546,\n",
       " 'be': 2383,\n",
       " 'up': 16895,\n",
       " 'class': 3774,\n",
       " 'wounds': 17822,\n",
       " 'lake': 9485,\n",
       " 'highlands': 7724,\n",
       " 'crack': 4242,\n",
       " 'the': 16022,\n",
       " 'path': 12228,\n",
       " 'where': 17582,\n",
       " 'wiped': 17684,\n",
       " 'this': 16107,\n",
       " 'morning': 10886,\n",
       " 'during': 5348,\n",
       " 'beach': 2386,\n",
       " 'run': 14049,\n",
       " 'surface': 15606,\n",
       " 'on': 11828,\n",
       " 'elbow': 5571,\n",
       " 'and': 1651,\n",
       " 'right': 13783,\n",
       " 'knee': 9316,\n",
       " 'yaqrsximph': 18081,\n",
       " 'airplane': 1458,\n",
       " '20accident': 309,\n",
       " 'somewhere': 15025,\n",
       " 'there': 16070,\n",
       " 'experts': 5988,\n",
       " 'france': 6594,\n",
       " 'begin': 2442,\n",
       " 'examining': 5934,\n",
       " 'debris': 4641,\n",
       " 'found': 6553,\n",
       " 'reunion': 13707,\n",
       " 'island': 8601,\n",
       " 'french': 6642,\n",
       " 'air': 1451,\n",
       " 'accident': 1241,\n",
       " 'tagzbcxfj0': 15758,\n",
       " 'mlb': 10762,\n",
       " 'bloody': 2719,\n",
       " 'isolated': 8610,\n",
       " 'city': 3742,\n",
       " 'world': 17794,\n",
       " 'perth': 12370,\n",
       " 'came': 3251,\n",
       " 'kill': 9243,\n",
       " 'indians': 8356,\n",
       " 'fun': 6721,\n",
       " 'video': 17164,\n",
       " 'smirking': 14922,\n",
       " 'remorseless': 13579,\n",
       " 'pakistani': 12124,\n",
       " 'killer': 9247,\n",
       " 'shows': 14678,\n",
       " 'him': 7746,\n",
       " 'boasting': 2766,\n",
       " 'fpjlwoxklg': 6575,\n",
       " 'burning': 3101,\n",
       " 'johnsontionne': 8903,\n",
       " 'except': 5939,\n",
       " 'idk': 8176,\n",
       " 'them': 16055,\n",
       " 'it': 8620,\n",
       " 'really': 13423,\n",
       " 'destroy': 4823,\n",
       " 'he': 7580,\n",
       " 'or': 11919,\n",
       " 'she': 14590,\n",
       " 'her': 7670,\n",
       " 'ask': 1943,\n",
       " 'house': 7926,\n",
       " 'wounded': 17821,\n",
       " 'maracay': 10278,\n",
       " 'nirgua': 11374,\n",
       " 'venezuela': 17104,\n",
       " 'officer': 11725,\n",
       " 'dead': 4615,\n",
       " 'after': 1395,\n",
       " 'exchanging': 5943,\n",
       " 'shots': 14660,\n",
       " 'xxfk4khbiw': 18027,\n",
       " 'wreck': 17845,\n",
       " 'currently': 4410,\n",
       " 'writing': 17858,\n",
       " 'book': 2805,\n",
       " 'friggin': 6666,\n",
       " 'destiel': 4817,\n",
       " 'sucks': 15514,\n",
       " 'read': 13402,\n",
       " 'vine': 17185,\n",
       " 'description': 4797,\n",
       " 'https': 7971,\n",
       " 'mkx6ux4ozt': 10760,\n",
       " 'mudslide': 10998,\n",
       " 'malibu': 10234,\n",
       " 'santafe': 14210,\n",
       " 'winning': 17679,\n",
       " 'sterling': 15340,\n",
       " 'scott': 14326,\n",
       " 'red': 13475,\n",
       " 'carpet': 3368,\n",
       " 'fundraiser': 6724,\n",
       " 'oso': 11969,\n",
       " 'ma4ra7atql': 10151,\n",
       " 'cg579wldne': 3513,\n",
       " 'casualties': 3406,\n",
       " 'canadian': 3282,\n",
       " 'bread': 2915,\n",
       " 'libertarianluke': 9733,\n",
       " 'all': 1539,\n",
       " 'that': 16016,\n",
       " 'honest': 7859,\n",
       " 'if': 8187,\n",
       " 'people': 12330,\n",
       " 'want': 17376,\n",
       " 'go': 7080,\n",
       " 'rampage': 13313,\n",
       " 'let': 9698,\n",
       " 'use': 16955,\n",
       " 'their': 16050,\n",
       " 'own': 12051,\n",
       " 'hands': 7465,\n",
       " 'feet': 6221,\n",
       " 'no': 11417,\n",
       " 'ambulance': 1608,\n",
       " 'amsterdam': 1635,\n",
       " '7xglah10zl': 928,\n",
       " 'twelve': 16646,\n",
       " 'feared': 6198,\n",
       " 'helicopter': 7646,\n",
       " 'crash': 4257,\n",
       " 'thmblaatzp': 16115,\n",
       " 'electrocuted': 5585,\n",
       " 'got': 7141,\n",
       " 'last': 9543,\n",
       " 'night': 11359,\n",
       " 'work': 17784,\n",
       " 'first': 6348,\n",
       " 'time': 16206,\n",
       " 'my': 11069,\n",
       " 'life': 9746,\n",
       " 'shit': 14630,\n",
       " 'was': 17406,\n",
       " 'weird': 17514,\n",
       " 'drown': 5276,\n",
       " 'some': 15013,\n",
       " 'older': 11801,\n",
       " 'native': 11173,\n",
       " 'australians': 2061,\n",
       " 'believe': 2469,\n",
       " 'oceans': 11685,\n",
       " 'were': 17532,\n",
       " 'created': 4278,\n",
       " 'from': 6673,\n",
       " 'urine': 16932,\n",
       " 'an': 1638,\n",
       " 'angry': 1676,\n",
       " 'god': 7090,\n",
       " 'who': 17605,\n",
       " 'tried': 16503,\n",
       " 'volcano': 17257,\n",
       " 'west': 17537,\n",
       " 'coast': 3869,\n",
       " 'cali': 3226,\n",
       " 'usa': 16943,\n",
       " 'architect': 1834,\n",
       " 'behind': 2454,\n",
       " 'kanye': 9086,\n",
       " 'musbik7ejf': 11030,\n",
       " 'attack': 2002,\n",
       " 'mumbai': 11017,\n",
       " 'india': 8349,\n",
       " 'shud': 14684,\n",
       " 'not': 11481,\n",
       " 'give': 7013,\n",
       " 'any': 1734,\n",
       " 'evidence': 5919,\n",
       " 'pak': 12122,\n",
       " 'they': 16089,\n",
       " 'will': 17641,\n",
       " 'share': 14573,\n",
       " 'with': 17700,\n",
       " 'terrorists': 15962,\n",
       " 'amp': 1630,\n",
       " 'next': 11313,\n",
       " 'oth': 11977,\n",
       " 'contries': 4114,\n",
       " 'qiopbtiuvu': 13126,\n",
       " 'body': 2774,\n",
       " '20bag': 311,\n",
       " 'new': 11281,\n",
       " 'york': 18173,\n",
       " 'auth': 2063,\n",
       " 'louis': 9993,\n",
       " 'vuitton': 17294,\n",
       " 'brown': 3002,\n",
       " 'saumur': 14245,\n",
       " '35': 485,\n",
       " 'cross': 4322,\n",
       " 'shoulder': 14662,\n",
       " 'bag': 2202,\n",
       " 'monogram': 10842,\n",
       " '23': 355,\n",
       " '419': 581,\n",
       " 'full': 6718,\n",
       " 'û_': 18408,\n",
       " 'hcdiwe5flc': 7571,\n",
       " 'zlvebeoavg': 18338,\n",
       " 'annihilated': 1696,\n",
       " 'higher': 7721,\n",
       " 'places': 12532,\n",
       " 'episode': 5783,\n",
       " 'trunks': 16557,\n",
       " 'freiza': 6640,\n",
       " 'is': 8585,\n",
       " 'cleanest': 3782,\n",
       " 'ever': 5902,\n",
       " 'showed': 14674,\n",
       " 'nigga': 11357,\n",
       " 'mercy': 10518,\n",
       " 'cyclone': 4447,\n",
       " 'hyderabad': 8073,\n",
       " 'roughdeal1': 13969,\n",
       " 'ante': 1714,\n",
       " 'hudhud': 7978,\n",
       " 'chandrababu': 3544,\n",
       " 'valle': 17039,\n",
       " 'ne': 11224,\n",
       " 'ga': 6799,\n",
       " '65': 781,\n",
       " 'zhenghxn': 18312,\n",
       " '11': 117,\n",
       " 'eyes': 6036,\n",
       " 'akame': 1474,\n",
       " 'tokyo': 16296,\n",
       " 'ghoul': 6978,\n",
       " 'damn': 4511,\n",
       " 'dont': 5154,\n",
       " 'dare': 4551,\n",
       " 'watch': 17423,\n",
       " 'suicide': 15530,\n",
       " '20bombing': 317,\n",
       " 'principality': 12842,\n",
       " 'zeron': 18304,\n",
       " 'rayquazaerk': 13368,\n",
       " 'are': 1836,\n",
       " 'christian': 3689,\n",
       " 'sure': 15600,\n",
       " 'but': 3126,\n",
       " 'don': 5144,\n",
       " 'bombing': 2794,\n",
       " 'employed': 5668,\n",
       " 'often': 11746,\n",
       " 'as': 1912,\n",
       " 'islamic': 8598,\n",
       " 'groups': 7259,\n",
       " 'demolished': 4739,\n",
       " 'jackmulholland1': 8698,\n",
       " 'think': 16099,\n",
       " 'also': 1581,\n",
       " 'became': 2412,\n",
       " 'marquis': 10322,\n",
       " 'then': 16062,\n",
       " 'carlos': 3355,\n",
       " 'charlie': 3569,\n",
       " 'finally': 6308,\n",
       " 'dublin': 5308,\n",
       " 'sadly': 14130,\n",
       " 'inundated': 8515,\n",
       " 'surf': 15604,\n",
       " 'hi': 7711,\n",
       " 'waimea': 17339,\n",
       " 'bay': 2344,\n",
       " 'like': 9769,\n",
       " 'surfers': 15607,\n",
       " 'czdw8oowa2': 4460,\n",
       " 'collision': 3928,\n",
       " 'denver': 4763,\n",
       " 'colorado': 3939,\n",
       " 'motorcyclist': 10919,\n",
       " 'bicyclist': 2564,\n",
       " 'injured': 8415,\n",
       " 'broadway': 2984,\n",
       " 'zl7ojdaj3u': 18334,\n",
       " 'flames': 6386,\n",
       " 'around': 1879,\n",
       " 'you': 18177,\n",
       " 'maryland': 10334,\n",
       " 'mansion': 10268,\n",
       " 'fire': 6330,\n",
       " 'caused': 3434,\n",
       " 'damaged': 4502,\n",
       " 'plug': 12586,\n",
       " 'under': 16813,\n",
       " 'christmas': 3696,\n",
       " 'tree': 16469,\n",
       " 'report': 13616,\n",
       " 'says': 14264,\n",
       " 'into': 8509,\n",
       " 'lkjfabqzb3': 9869,\n",
       " 'demolish': 4738,\n",
       " 'nyhc': 11603,\n",
       " 'going': 7098,\n",
       " 'drake': 5213,\n",
       " 'over': 12023,\n",
       " 'ghostwriting': 6977,\n",
       " 'should': 14661,\n",
       " 'know': 9331,\n",
       " 'rihanna': 13788,\n",
       " 'lives': 9847,\n",
       " 'door': 5158,\n",
       " 'buildings': 3066,\n",
       " '20burning': 319,\n",
       " 'blue': 2736,\n",
       " 'yes': 18118,\n",
       " '1acd4900c1424d1': 232,\n",
       " 'foxnews': 6567,\n",
       " 'one': 11834,\n",
       " 'down': 5182,\n",
       " 'looting': 9961,\n",
       " 'forest': 6518,\n",
       " '20fires': 327,\n",
       " 'nicola': 11345,\n",
       " 'valley': 17040,\n",
       " 'fires': 6343,\n",
       " 'dying': 5384,\n",
       " 'salmon': 14165,\n",
       " 'act': 1279,\n",
       " 'deny': 4764,\n",
       " 'climate': 3805,\n",
       " 'change': 3545,\n",
       " 'nightmares': 11362,\n",
       " 'here': 7671,\n",
       " 'rbzomwgjee': 13380,\n",
       " 'bcpoli': 2375,\n",
       " 'canpoli': 3300,\n",
       " 'vanpoli': 17056,\n",
       " 'ns1aggfnxz': 11533,\n",
       " 'shoes': 14642,\n",
       " 'asics': 1940,\n",
       " 'gt': 7281,\n",
       " 'ii': 8217,\n",
       " 'super': 15573,\n",
       " 'ronnie': 13935,\n",
       " 'fieg': 6275,\n",
       " 'kith': 9287,\n",
       " 'white': 17599,\n",
       " '3m': 536,\n",
       " 'gel': 6908,\n",
       " 'grey': 7233,\n",
       " 'od250zshfy': 11689,\n",
       " 'sandstorm': 14201,\n",
       " 'airport': 1460,\n",
       " 'get': 6950,\n",
       " 'swallowed': 15645,\n",
       " 'minute': 10695,\n",
       " 'wd9odwjj9l': 17467,\n",
       " '20on': 331,\n",
       " '20fire': 326,\n",
       " 'uk': 16776,\n",
       " 'tweetlikeitsseptember11th2001': 16643,\n",
       " 'those': 16126,\n",
       " 'two': 16658,\n",
       " 'oil': 11769,\n",
       " '20spill': 338,\n",
       " 'ny': 11592,\n",
       " 'california': 3229,\n",
       " 'spill': 15149,\n",
       " 'might': 10622,\n",
       " 'larger': 9535,\n",
       " 'than': 16005,\n",
       " 'projected': 12889,\n",
       " 'xwxbyhtuzc': 18026,\n",
       " 'wzedxefblg': 17918,\n",
       " 'cartoon': 3380,\n",
       " 'bears': 2394,\n",
       " 'without': 17705,\n",
       " 'we': 17472,\n",
       " 'would': 17817,\n",
       " 'qave': 13098,\n",
       " 'knowlddge': 9333,\n",
       " 'toilet': 16293,\n",
       " 'paper': 12162,\n",
       " 'drought': 5271,\n",
       " 'miami': 10585,\n",
       " '_gaabyx': 1137,\n",
       " 'purple': 13035,\n",
       " 'activist': 1291,\n",
       " 'thought': 16129,\n",
       " 'injuries': 8417,\n",
       " 'madison': 10177,\n",
       " 'wi': 17620,\n",
       " 'st': 15242,\n",
       " 'mo': 10781,\n",
       " 'buffoonmike': 3060,\n",
       " 'knew': 9320,\n",
       " 'doing': 5128,\n",
       " 'much': 10995,\n",
       " 'bite': 2628,\n",
       " 'us': 16939,\n",
       " 'influenced': 8398,\n",
       " 'shitty': 14634,\n",
       " 'staff': 15251,\n",
       " 'acquisitions': 1274,\n",
       " 'landslide': 9511,\n",
       " 'austin': 2058,\n",
       " 'texas': 15977,\n",
       " 'toddstarnes': 16285,\n",
       " 'enjoy': 5729,\n",
       " 'impending': 8289,\n",
       " 'todd': 16283,\n",
       " 'hehe': 7632,\n",
       " 'apocalypse': 1768,\n",
       " 'oregon': 11935,\n",
       " 'look': 9946,\n",
       " 'grizzly': 7245,\n",
       " 'peak': 12297,\n",
       " 'now': 11512,\n",
       " 'looks': 9950,\n",
       " 'beginning': 2444,\n",
       " 'dystopian': 5389,\n",
       " 'detonation': 4841,\n",
       " 'ignition': 8203,\n",
       " 'knock': 9326,\n",
       " 'sensor': 14469,\n",
       " 'senso': 14468,\n",
       " 'standard': 15264,\n",
       " 'ks100': 9383,\n",
       " '7o4lnfbe7k': 920,\n",
       " 'fvzsgjtbew': 6753,\n",
       " '20responders': 334,\n",
       " 'week': 17505,\n",
       " 'responders': 13667,\n",
       " 'dart': 4564,\n",
       " 'members': 10495,\n",
       " 'participating': 12201,\n",
       " 'four': 6558,\n",
       " 'day': 4593,\n",
       " 'intensive': 8481,\n",
       " 'technical': 15886,\n",
       " 'large': 9534,\n",
       " 'animal': 1678,\n",
       " 'tl93aod3er': 16247,\n",
       " 'military': 10641,\n",
       " 'lot': 9984,\n",
       " '20': 281,\n",
       " 'tom': 16302,\n",
       " 'clancy': 3770,\n",
       " 'mystery': 11085,\n",
       " 'novels': 11510,\n",
       " 'paperback': 12163,\n",
       " 'obix79ncxn': 11654,\n",
       " 'tomclancy': 16306,\n",
       " 'drowning': 5278,\n",
       " 'coventry': 4216,\n",
       " 'why': 17618,\n",
       " 'low': 10009,\n",
       " 'self': 14439,\n",
       " 'image': 8264,\n",
       " 'take': 15766,\n",
       " 'quiz': 13198,\n",
       " 'z8r6r3nbtb': 18264,\n",
       " 'namffldh5h': 11138,\n",
       " 'gonna': 7110,\n",
       " 'fight': 6284,\n",
       " 'taylor': 15834,\n",
       " 'soon': 15044,\n",
       " 'danger': 4525,\n",
       " 'hailing': 7424,\n",
       " 'dayton': 4595,\n",
       " 'wish': 17695,\n",
       " 'could': 4190,\n",
       " 'victoria': 17154,\n",
       " 'secret': 14399,\n",
       " 'front': 6674,\n",
       " 'good': 7111,\n",
       " 'flood': 6428,\n",
       " 'spot': 15181,\n",
       " 'combo': 3950,\n",
       " '53inch': 689,\n",
       " '300w': 459,\n",
       " 'curved': 4414,\n",
       " 'cree': 4288,\n",
       " 'led': 9634,\n",
       " 'light': 9757,\n",
       " 'bar': 2268,\n",
       " '4x4': 659,\n",
       " 'offroad': 11739,\n",
       " 'fog': 6467,\n",
       " 'lamp': 9493,\n",
       " 're': 13389,\n",
       " 'o097vsotxk': 11619,\n",
       " 'i23xy7iejj': 8089,\n",
       " 'severe': 14522,\n",
       " 'weather': 17486,\n",
       " 'bulletin': 3073,\n",
       " 'typhoon': 16684,\n",
       " 'ûï': 18429,\n",
       " 'hannaph': 7473,\n",
       " 'soudelor': 15063,\n",
       " 'tropical': 16532,\n",
       " 'warning': 17391,\n",
       " 'issued': 8615,\n",
       " '00': 0,\n",
       " 'pm': 12594,\n",
       " '06': 23,\n",
       " 'thhjjw51pe': 16092,\n",
       " 'fits': 6357,\n",
       " '01': 6,\n",
       " 'bmw': 2754,\n",
       " '325ci': 478,\n",
       " '5l': 726,\n",
       " 'l6': 9449,\n",
       " 'gbvdnczjou': 6893,\n",
       " 'c211hise0r': 3169,\n",
       " 'explosion': 6003,\n",
       " 'london': 9928,\n",
       " 'united': 16851,\n",
       " 'kingdom': 9267,\n",
       " '10': 87,\n",
       " 'chemical': 3605,\n",
       " 'park': 12186,\n",
       " 'western': 17541,\n",
       " 'germany': 6945,\n",
       " 'xbznu0qkvs': 17948,\n",
       " 'bomb': 2789,\n",
       " 'soul': 15065,\n",
       " 'food': 6488,\n",
       " 'sound': 15069,\n",
       " 'so': 14969,\n",
       " 'terrorism': 15960,\n",
       " 'truth': 16561,\n",
       " 'bejftygjil': 2461,\n",
       " 'news': 11295,\n",
       " 'bbc': 2353,\n",
       " 'islam': 8595,\n",
       " 'isis': 8593,\n",
       " 'quran': 13207,\n",
       " 'lies': 9745,\n",
       " 'jlczidz7vu': 8866,\n",
       " 'sinking': 14765,\n",
       " 'your': 18187,\n",
       " 'lost': 9983,\n",
       " 'alone': 1569,\n",
       " 'stone': 15374,\n",
       " 'carry': 3373,\n",
       " 'onå': 11862,\n",
       " 'hostage': 7911,\n",
       " 'chicago': 3624,\n",
       " 'mylittlepwnies3': 11080,\n",
       " 'early__may': 5428,\n",
       " 'anathemazhiv': 1645,\n",
       " 'tonysandos': 16320,\n",
       " 'which': 17587,\n",
       " 'has': 7522,\n",
       " 'do': 5103,\n",
       " 'lebanon': 9631,\n",
       " '80s': 943,\n",
       " 'iran': 8561,\n",
       " 'crisis': 4311,\n",
       " 'libya': 9740,\n",
       " 'pan': 12140,\n",
       " 'am': 1595,\n",
       " 'pa': 12092,\n",
       " 'pulls': 13002,\n",
       " 'gun': 7315,\n",
       " 'man': 10245,\n",
       " 'apparent': 1779,\n",
       " 'provocation': 12948,\n",
       " 'lhw4vtbhzg': 9729,\n",
       " 'via': 17141,\n",
       " 'dailykos': 4488,\n",
       " 'derailment': 4784,\n",
       " 'minneapolis': 10684,\n",
       " 'mn': 10776,\n",
       " 'train': 16424,\n",
       " 'patna': 12236,\n",
       " 'casualty': 3407,\n",
       " 'far': 6130,\n",
       " 'indian': 8352,\n",
       " 'express': 6011,\n",
       " 'yh5vetm0yz': 18131,\n",
       " '17wgug8z0m': 197,\n",
       " 'panic': 12151,\n",
       " 'dream': 5228,\n",
       " 'magic': 10189,\n",
       " 'linden': 9791,\n",
       " 'method': 10548,\n",
       " 'lite': 9821,\n",
       " 'version': 17119,\n",
       " 'anxiety': 1733,\n",
       " 'cure': 4403,\n",
       " 'program': 12880,\n",
       " '073izwx0lb': 28,\n",
       " 'lind': 9789,\n",
       " 'okmlagvkjv': 11790,\n",
       " 'rescued': 13638,\n",
       " 'jammu': 8730,\n",
       " 'kashmir': 9098,\n",
       " 'delhi': 4711,\n",
       " '18': 198,\n",
       " 'bovines': 2849,\n",
       " 'smugglersåênabbed': 14936,\n",
       " 'e7fn5g5ruu': 5411,\n",
       " 'fredericksburg': 6615,\n",
       " 'virginia': 17202,\n",
       " 'wwp': 17901,\n",
       " 'serving': 14504,\n",
       " 'more': 10873,\n",
       " '75k': 881,\n",
       " 'veterans': 17126,\n",
       " '52k': 684,\n",
       " 'oif': 11768,\n",
       " 'oef': 11702,\n",
       " 'vets': 17127,\n",
       " 'physical': 12441,\n",
       " 'many': 10271,\n",
       " 'invisible': 8530,\n",
       " 'ones': 11840,\n",
       " 'shhlv4dplz': 14610,\n",
       " 'client': 3801,\n",
       " 'dust': 5351,\n",
       " '20storm': 339,\n",
       " 'learned': 9622,\n",
       " 'about': 1209,\n",
       " 'economics': 5479,\n",
       " 'south': 15079,\n",
       " 'dakota': 4492,\n",
       " 'storm': 15389,\n",
       " 'did': 4909,\n",
       " 'years': 18101,\n",
       " 'college': 3922,\n",
       " 'hubert': 7977,\n",
       " 'humphrey': 8012,\n",
       " 'wrecked': 17847,\n",
       " 'cramer': 4250,\n",
       " 'iger': 8195,\n",
       " 'words': 17783,\n",
       " 'disney': 5008,\n",
       " 'stock': 15365,\n",
       " 'sf5jdnvdw9': 14534,\n",
       " 'til_now': 16200,\n",
       " 'cnbc': 3849,\n",
       " 'tring': 16510,\n",
       " 'marc_holl': 10281,\n",
       " 'nennicook': 11260,\n",
       " 'aitchkaycee': 1465,\n",
       " 'vixstuart': 17228,\n",
       " 'benjbeckwith': 2497,\n",
       " 'pretty': 12817,\n",
       " 'disaster': 4979,\n",
       " 'gbbo': 6886,\n",
       " 'obliteration': 11663,\n",
       " 'canada': 3280,\n",
       " 'need': 11240,\n",
       " 'arcade': 1832,\n",
       " 'shooter': 14646,\n",
       " 'fix': 6362,\n",
       " 'cte': 4367,\n",
       " 'empty': 5673,\n",
       " 'only': 11850,\n",
       " 'running': 14056,\n",
       " 'even': 5896,\n",
       " 'buy': 3138,\n",
       " 'cod': 3885,\n",
       " 'title': 16234,\n",
       " 'weren': 17533,\n",
       " 'overpriced': 12032,\n",
       " 'steam': 15324,\n",
       " 'bioterrorism': 2609,\n",
       " 'firepower': 6342,\n",
       " 'lab': 9462,\n",
       " 'electronic': 5586,\n",
       " 'resource': 13659,\n",
       " 'automation': 2075,\n",
       " 'against': 1409,\n",
       " 'infectious': 8386,\n",
       " 'diseases': 4998,\n",
       " 'kvpbybglsr': 9415,\n",
       " 'graysondolan': 7201,\n",
       " 'me': 10434,\n",
       " 'explode': 5995,\n",
       " 'washington': 17412,\n",
       " 'kendall': 9160,\n",
       " 'jenner': 8806,\n",
       " 'nick': 11339,\n",
       " 'jonas': 8915,\n",
       " 'dating': 4573,\n",
       " 'quite': 13196,\n",
       " 'literally': 9822,\n",
       " 'pfvzvpxqgr': 12399,\n",
       " 'always': 1594,\n",
       " 'tell': 15913,\n",
       " 'mom': 10823,\n",
       " 'bring': 2963,\n",
       " 'hold': 7813,\n",
       " 'cat': 3408,\n",
       " 'heat': 7616,\n",
       " '20wave': 342,\n",
       " 'fort': 6540,\n",
       " 'worth': 17816,\n",
       " 'rt': 14016,\n",
       " 'startelegram': 15290,\n",
       " 'homeless': 7842,\n",
       " 'vulnerable': 17297,\n",
       " 'north': 11463,\n",
       " 'wave': 17440,\n",
       " 'k9airfq3ql': 9049,\n",
       " 'jdbtlymehy': 8790,\n",
       " 'nuclear': 11547,\n",
       " '20reactor': 333,\n",
       " 'solar': 14997,\n",
       " 'power': 12718,\n",
       " 'japanese': 8748,\n",
       " 'fukushima': 6717,\n",
       " 'reactor': 13399,\n",
       " 'energy': 5710,\n",
       " 'japan': 8747,\n",
       " 'temperature': 15924,\n",
       " 'fuel': 6710,\n",
       " 'pool': 12655,\n",
       " 'ys3nmwwyvc': 18215,\n",
       " 'alpotnb7q3': 1574,\n",
       " 'arvada': 1908,\n",
       " 'least': 9626,\n",
       " 'taken': 15769,\n",
       " 'local': 9900,\n",
       " 'wlmsq3mtho': 17725,\n",
       " 'trauma': 16451,\n",
       " 'nashville': 11161,\n",
       " 'tn': 16264,\n",
       " 'esteemed': 5852,\n",
       " 'journalist': 8936,\n",
       " 'recalls': 13445,\n",
       " 'tragic': 16420,\n",
       " 'effects': 5529,\n",
       " 'unaddressed': 16797,\n",
       " 'childhood': 3636,\n",
       " 'keithboykin': 9149,\n",
       " 'randallpinkston': 13317,\n",
       " 'pozarmy': 12723,\n",
       " 'gxq1auzb18': 7362,\n",
       " 'panicking': 12152,\n",
       " 'feel': 6213,\n",
       " 'results': 13689,\n",
       " 'back': 2179,\n",
       " 'alarmingly': 1498,\n",
       " 'calm': 3236,\n",
       " 'lightning': 9763,\n",
       " 'thunder': 16168,\n",
       " 'possible': 12689,\n",
       " 'pinpoint': 12479,\n",
       " 'foothill': 6502,\n",
       " 'forecast': 6511,\n",
       " 'ctijdpxabk': 4369,\n",
       " 'displaced': 5019,\n",
       " '40': 562,\n",
       " 'ocean': 11684,\n",
       " 'township': 16385,\n",
       " 'apartment': 1756,\n",
       " 'newyork': 11308,\n",
       " 'uelz59wvom': 16749,\n",
       " 'massacre': 10345,\n",
       " 'stay': 15315,\n",
       " 'tuned': 16602,\n",
       " 'freddiedeboer': 6612,\n",
       " 'thucydiplease': 16166,\n",
       " 'rise': 13810,\n",
       " 'coates': 3874,\n",
       " 'charleston': 3568,\n",
       " 'walter': 17366,\n",
       " 'black': 2652,\n",
       " 'twitter': 16656,\n",
       " 'broadly': 2982,\n",
       " 'well': 17519,\n",
       " 'deaths': 4636,\n",
       " 'gallifrey': 6826,\n",
       " 'mathew_is_angry': 10366,\n",
       " 'z3ke_sk1': 18256,\n",
       " 'saladinahmed': 14154,\n",
       " 'died': 4915,\n",
       " 'horrible': 7898,\n",
       " 'trapped': 16448,\n",
       " 'ships': 14624,\n",
       " 'risk': 13813,\n",
       " '20buildings': 318,\n",
       " 'whiterun': 17601,\n",
       " 'skyrim': 14840,\n",
       " 'destruction': 4829,\n",
       " 'fine': 6315,\n",
       " 'just': 9003,\n",
       " 'windstorm': 17667,\n",
       " 'palm': 12132,\n",
       " 'county': 4202,\n",
       " 'fl': 6379,\n",
       " 'reality': 13418,\n",
       " 'training': 16426,\n",
       " 'falls': 6103,\n",
       " 'off': 11716,\n",
       " 'elevated': 5597,\n",
       " 'tracks': 16405,\n",
       " 'jiomnrcygt': 8854,\n",
       " 'paramedic': 12173,\n",
       " 'ems': 5675,\n",
       " 'rescuers': 13639,\n",
       " 'fears': 6199,\n",
       " 'missing': 10721,\n",
       " 'migrants': 10625,\n",
       " 'med': 10448,\n",
       " 'search': 14378,\n",
       " 'survivors': 15627,\n",
       " 'boat': 2767,\n",
       " 'carrying': 3375,\n",
       " '6ds67xai5e': 810,\n",
       " 'derailed': 4782,\n",
       " 'toronto': 16346,\n",
       " 'derailed_benchmark': 4783,\n",
       " 'cool': 4127,\n",
       " 'paths': 12232,\n",
       " 'wonder': 17762,\n",
       " 'can': 3277,\n",
       " 'find': 6312,\n",
       " 'leaks': 9619,\n",
       " 'jobs': 8885,\n",
       " 'given': 7016,\n",
       " 'resque': 13673,\n",
       " 'too': 16321,\n",
       " 'ladies': 9473,\n",
       " 'tote': 16362,\n",
       " 'handbag': 7458,\n",
       " 'women': 17757,\n",
       " 'faux': 6162,\n",
       " 'leather': 9627,\n",
       " 'fashion': 6144,\n",
       " 'purse': 13040,\n",
       " 'y87gi3brlv': 18057,\n",
       " '1zbhvdcxzs': 280,\n",
       " 'raishimi33': 13302,\n",
       " 'sounds': 15073,\n",
       " 'plan': 12537,\n",
       " 'little': 9827,\n",
       " 'applaud': 1785,\n",
       " 'catastrophic': 3414,\n",
       " 'buxton': 3137,\n",
       " 'venice': 17105,\n",
       " 'nottingham': 11502,\n",
       " 'invading': 8518,\n",
       " 'iraq': 8566,\n",
       " 'mistake': 10730,\n",
       " 'diplomacy': 4951,\n",
       " 'needs': 11244,\n",
       " 'replace': 13611,\n",
       " 'constant': 4084,\n",
       " 'threat': 16137,\n",
       " 'war': 17380,\n",
       " 'israel': 8612,\n",
       " 'yqjpn3quux': 18205,\n",
       " 'related': 13543,\n",
       " 'threatens': 16140,\n",
       " 'europe': 5881,\n",
       " 'wk6b5z803o': 17720,\n",
       " 'livingston': 9852,\n",
       " 'mt': 10982,\n",
       " 'marynmck': 10335,\n",
       " 'beyond': 2537,\n",
       " 'adorable': 1334,\n",
       " 'hope': 7882,\n",
       " 'won': 17761,\n",
       " 'been': 2431,\n",
       " 'noticed': 11491,\n",
       " 'devastation': 4849,\n",
       " 'mount': 10923,\n",
       " 'vernon': 17115,\n",
       " 'coming': 3962,\n",
       " 'target': 15803,\n",
       " 'starbucks': 15279,\n",
       " 'closed': 3819,\n",
       " 'momneedscoffee': 10827,\n",
       " 'asap': 1915,\n",
       " 'iwontmakeit': 8651,\n",
       " 'bombed': 2791,\n",
       " 'screwston': 14353,\n",
       " 'tx': 16664,\n",
       " 'redskins': 13491,\n",
       " 'wr': 17838,\n",
       " 'roberts': 13868,\n",
       " 'belly': 2478,\n",
       " 'teamstream': 15877,\n",
       " 'gbcvvevdty': 6887,\n",
       " 'hellfire': 7650,\n",
       " 'allah': 1540,\n",
       " 'describes': 4795,\n",
       " 'piling': 12470,\n",
       " 'wealth': 17478,\n",
       " 'thinking': 16100,\n",
       " 'forever': 6519,\n",
       " 'surah': 15599,\n",
       " 'humaza': 8003,\n",
       " 'reflect': 13505,\n",
       " 'worldwide': 17802,\n",
       " 'loved': 9998,\n",
       " 'way': 17445,\n",
       " 'written': 17861,\n",
       " 'include': 8327,\n",
       " 'vantage': 17057,\n",
       " 'points': 12625,\n",
       " 'detkenlang': 4838,\n",
       " 'kindle': 9259,\n",
       " 'kcrnmjkj73': 9131,\n",
       " 'heartdisease': 7609,\n",
       " 'service': 14501,\n",
       " 'spending': 15140,\n",
       " 'half': 7436,\n",
       " 'budget': 3052,\n",
       " 'kzfigkeeva': 9434,\n",
       " 'tragedy': 16419,\n",
       " 'sandrabland': 14200,\n",
       " 'forget': 6522,\n",
       " 'gajtugaui7': 6817,\n",
       " 'mayhem': 10395,\n",
       " 'detroit': 4842,\n",
       " 'liked': 9770,\n",
       " 'youtube': 18195,\n",
       " 'itsjustinstuart': 8632,\n",
       " 'mnkaji2q1n': 10777,\n",
       " 'range': 13325,\n",
       " 'hungerarticles': 8019,\n",
       " 'nepal': 11264,\n",
       " 'rebuilding': 13440,\n",
       " 'livelihoods': 9839,\n",
       " 'quake': 13161,\n",
       " 'lrouwjmbix': 10026,\n",
       " 'arsonist': 1892,\n",
       " 'suspected': 15633,\n",
       " 'serial': 14492,\n",
       " 'arrested': 1882,\n",
       " 'calif': 3228,\n",
       " 'pzotpdgaki': 13075,\n",
       " 'hurricane': 8036,\n",
       " 'vineyard': 17187,\n",
       " 'chubbysquirrel_': 3702,\n",
       " 'hurricane_surge': 8039,\n",
       " ...}"
      ]
     },
     "execution_count": 1146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = cnt_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_analyse(dict):\n",
    "    vocabulary_keys = list(vocabulary.keys())\n",
    "    numbers = 0\n",
    "    punct = 0\n",
    "    hashtag = 0\n",
    "    mention = 0\n",
    "    reg_num = r'\\d+'\n",
    "    reg_punct = '[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+'\n",
    "    reg_hashtag = '^#\\w+'\n",
    "    reg_mention = '^@\\w+'\n",
    "\n",
    "    for i in range(len(vocabulary_keys)):\n",
    "        if re.findall(reg_num, vocabulary_keys[i]) != []:\n",
    "            numbers += 1\n",
    "        if re.findall(reg_punct, vocabulary_keys[i]) != []:\n",
    "            punct += 1\n",
    "        if re.findall(reg_hashtag, vocabulary_keys[i]) != []:\n",
    "            hashtag += 1\n",
    "        if re.findall(reg_mention, vocabulary_keys[i]) != []:\n",
    "            mention += 1\n",
    "    return f'количество слов с числами: {numbers}, количество слов с пунктуацией: {punct}, количество слов с хэштэгом: {hashtag}, количество слов с упоминанием: {mention}'\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'количество слов с числами: 3812, количество слов с пунктуацией: 315, количество слов с хэштэгом: 0, количество слов с упоминанием: 0'"
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_analyse(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6 (0.5 балла)\n",
    "\n",
    "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "def tokenizer_tw(text):\n",
    "    tw_tok = TweetTokenizer()\n",
    "    return tw_tok.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec_tw = CountVectorizer(tokenizer = tokenizer_tw)\n",
    "X_t = cnt_vec_tw.fit_transform(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'количество слов с числами: 3939, количество слов с пунктуацией: 7337, количество слов с хэштэгом: 1468, количество слов с упоминанием: 1678'"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = cnt_vec_tw.vocabulary_\n",
    "dict_analyse(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TweetTokenizer гораздо более \"нежный\" токенайзер, он не чистит пунтуацию и сохраняет специфичные для твиттера лингвистические конструкты как токены"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (2 балла)\n",
    "\n",
    "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
    "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
    "\n",
    "0. Приведет все буквы к нижнему регистру\n",
    "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
    "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
    "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
    "4. Проведет стемминг с помощью SnowballStemmer\n",
    "\n",
    "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'my', 'younger', 'and', 'more', 'vulner', 'year', 'my', 'father', 'gave', 'me', 'some', 'advic', 'that', 'i', 'been', 'turn', 'over', 'in', 'my', 'mind', 'ever', 'sinc', 'whenev', 'you', 'feel', 'like', 'critic', 'ani', 'one', 'he', 'told', 'me', 'just', 'rememb', 'that', 'all', 'the', 'peopl', 'in', 'this', 'world', 'have', 'had', 'the', 'advantag', 'that', 'you', 'had']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "text = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\\n\\\"Whenever you feel like criticizing any one,\\\" he told me, \\\"just remember that all the people in this world haven't had the advantages that you've had.\\\"\"\n",
    "text_tokenized = [stemmer.stem(w) for w in word_tokenize(text) if w.isalpha()]\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'my', 'younger', 'and', 'more', 'vulner', 'year', 'my', 'father', 'gave', 'me', 'some', 'advic', 'that', 'i', 'been', 'turn', 'over', 'in', 'my', 'mind', 'ever', 'sinc', 'whenev', 'you', 'feel', 'like', 'critic', 'ani', 'one', 'he', 'told', 'me', 'just', 'rememb', 'that', 'all', 'the', 'peopl', 'in', 'this', 'world', 'have', 'had', 'the', 'advantag', 'that', 'you', 'had']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "text = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\\n\\\"Whenever you feel like criticizing any one,\\\" he told me, \\\"just remember that all the people in this world haven't had the advantages that you've had.\\\"\"\n",
    "text_tokenized = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
    "print(text_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tw = TweetTokenizer()\n",
    "    text_preprocessed = [stemmer.stem(w) for w in tw.tokenize(text.lower()) if (w.isalpha() \n",
    "    or re.findall('^#+[a-z]+$', w) != []\n",
    "    or (re.findall('[a-z0-9]', w) == [] and re.findall('\\W+[()]+', w) != [])\n",
    "    ) and w not in stop_words]\n",
    "     \n",
    "    \n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bridg',\n",
       "  'ash',\n",
       "  'australia',\n",
       "  'ûªs',\n",
       "  'collaps',\n",
       "  'trent',\n",
       "  'bridg',\n",
       "  'among',\n",
       "  'worst',\n",
       "  'histori',\n",
       "  'england',\n",
       "  'bundl',\n",
       "  'australia'],\n",
       " ['hail',\n",
       "  'carol',\n",
       "  'stream',\n",
       "  'illinoi',\n",
       "  'great',\n",
       "  'michigan',\n",
       "  'techniqu',\n",
       "  'camp',\n",
       "  'thank',\n",
       "  '#goblu',\n",
       "  '#wrestleon'],\n",
       " ['polic',\n",
       "  'houston',\n",
       "  'cnn',\n",
       "  'tennesse',\n",
       "  'movi',\n",
       "  'theater',\n",
       "  'shoot',\n",
       "  'suspect',\n",
       "  'kill',\n",
       "  'polic'],\n",
       " ['riot', 'still', 'riot', 'coupl', 'hour', 'left', 'class'],\n",
       " ['wound',\n",
       "  'lake',\n",
       "  'highland',\n",
       "  'crack',\n",
       "  'path',\n",
       "  'wipe',\n",
       "  'morn',\n",
       "  'beach',\n",
       "  'run',\n",
       "  'surfac',\n",
       "  'wound',\n",
       "  'left',\n",
       "  'elbow',\n",
       "  'right',\n",
       "  'knee'],\n",
       " ['airplan',\n",
       "  'somewher',\n",
       "  'expert',\n",
       "  'franc',\n",
       "  'begin',\n",
       "  'examin',\n",
       "  'airplan',\n",
       "  'debri',\n",
       "  'found',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accid',\n",
       "  'expert',\n",
       "  '#mlb'],\n",
       " ['bloodi',\n",
       "  'isol',\n",
       "  'citi',\n",
       "  'world',\n",
       "  'perth',\n",
       "  'came',\n",
       "  'kill',\n",
       "  'indian',\n",
       "  'fun',\n",
       "  'video',\n",
       "  'smirk',\n",
       "  'remorseless',\n",
       "  'pakistani',\n",
       "  'killer',\n",
       "  'show',\n",
       "  'boast'],\n",
       " ['burn', 'except', 'idk', 'realli', 'burn'],\n",
       " ['destroy', 'ask', 'destroy', 'hous'],\n",
       " ['wound',\n",
       "  'maracay',\n",
       "  'nirgua',\n",
       "  'venezuela',\n",
       "  'polic',\n",
       "  'offic',\n",
       "  'wound',\n",
       "  'suspect',\n",
       "  'dead',\n",
       "  'exchang',\n",
       "  'shot']]"
      ]
     },
     "execution_count": 1154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[custom_tokenizer(t) for t in train['text'][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 (1 балл)\n",
    "\n",
    "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n",
    "2. Обучите LogisticRegression на полученных признаках.\n",
    "3. Посчитайте метрику f1-score на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x10584 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 50857 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec_new = CountVectorizer(tokenizer = custom_tokenizer)\n",
    "X_train = cnt_vec_new.fit_transform(train['text'])\n",
    "X_test = cnt_vec_new.transform(test['text'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target']\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7524219590958019\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1318\n",
      "           1       0.78      0.72      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(f1_score(y_test, pred))\n",
    "print()\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 (1 балл)\n",
    "\n",
    "1. Повторите 7 задание, но с tf-idf векторизатором. Как изменилось качество?\n",
    "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n",
    "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x10584 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50857 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfifd_vec = TfidfVectorizer(tokenizer = custom_tokenizer)\n",
    "X_train_tf = tfifd_vec.fit_transform(train['text'])\n",
    "X_test_tf = tfifd_vec.transform(test['text'])\n",
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7437465258476932\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.83      1318\n",
      "           1       0.80      0.69      0.74       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.78      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train_tf, y_train)\n",
    "\n",
    "pred_tf = clf.predict(X_test_tf)\n",
    "print(f1_score(y_test, pred_tf))\n",
    "print()\n",
    "print(classification_report(y_test, pred_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество незначительно упало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x10584 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50857 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfifd_vec_new = TfidfVectorizer(tokenizer = custom_tokenizer, max_df=0.9)\n",
    "X_train_tf = tfifd_vec_new.fit_transform(train['text'])\n",
    "X_test_tf = tfifd_vec_new.transform(test['text'])\n",
    "X_train_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "размер не изменился, матрица осталась прежней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7437465258476932\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.83      1318\n",
      "           1       0.80      0.69      0.74       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.78      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_tf, y_train)\n",
    "\n",
    "pred_tf = clf.predict(X_test_tf)\n",
    "print(f1_score(y_test, pred_tf))\n",
    "print()\n",
    "print(classification_report(y_test, pred_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "удивительно, но ничего не изменилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x394 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20640 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfifd_vec_new_2 = TfidfVectorizer(tokenizer = custom_tokenizer, max_df=0.9, min_df = 0.005)\n",
    "X_train_tf = tfifd_vec_new_2.fit_transform(train['text'])\n",
    "X_test_tf = tfifd_vec_new_2.transform(test['text'])\n",
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71869918699187\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81      1318\n",
      "           1       0.75      0.69      0.72       966\n",
      "\n",
      "    accuracy                           0.77      2284\n",
      "   macro avg       0.77      0.76      0.76      2284\n",
      "weighted avg       0.77      0.77      0.77      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_tf, y_train)\n",
    "\n",
    "pred_tf = clf.predict(X_test_tf)\n",
    "print(f1_score(y_test, pred_tf))\n",
    "print()\n",
    "print(classification_report(y_test, pred_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица стала значительно меньше, ушло почти 10к признаков, качество упало уже ощутимее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 10 (1 балл)\n",
    "\n",
    "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
    "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
    "\n",
    "1. Повторите задание 7 с HashingVectorizer, укажите количество фичей равное 5000.\n",
    "2. Какой из подходов показал самый высокий результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlanamaslennikova/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5329x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50817 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_vec = HashingVectorizer(n_features = 5000, tokenizer = custom_tokenizer)\n",
    "X_train_hv = hash_vec.fit_transform(train['text'])\n",
    "X_test_hv = hash_vec.transform(test['text'])\n",
    "X_train_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7233333333333334\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82      1318\n",
      "           1       0.78      0.67      0.72       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.78      0.77      0.77      2284\n",
      "weighted avg       0.78      0.78      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train_hv, y_train)\n",
    "\n",
    "pred_hv = clf.predict(X_test_hv)\n",
    "print(f1_score(y_test, pred_hv))\n",
    "print()\n",
    "print(classification_report(y_test, pred_hv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, наибольший результат дал базовый bow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 11 (1 балл)\n",
    "\n",
    "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании 8 мы уже добились (!!!!) такого результата, интересно будет попробовать использовать не стемминг, а лемматизацию, пусть это и дольше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /Users/svetlanamaslennikova/opt/anaconda3/lib/python3.8/site-packages (0.9.1)\r\n",
      "Requirement already satisfied: docopt>=0.6 in /Users/svetlanamaslennikova/opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (0.6.2)\r\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/svetlanamaslennikova/opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (0.7.2)\r\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Users/svetlanamaslennikova/opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (2.4.417127.4579844)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer_lemma(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    pymorphy2_analyzer = MorphAnalyzer()\n",
    "    tw = TweetTokenizer()\n",
    "    text_preprocessed = [pymorphy2_analyzer.parse(w)[0].normal_form for w in tw.tokenize(text.lower()) if w not in stop_words]\n",
    "     \n",
    "    \n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bridge',\n",
       "  '%',\n",
       "  '20collapse',\n",
       "  'ashes',\n",
       "  '2015',\n",
       "  ':',\n",
       "  'australia',\n",
       "  '\\x89',\n",
       "  'ûªs',\n",
       "  'collapse',\n",
       "  'trent',\n",
       "  'bridge',\n",
       "  'among',\n",
       "  'worst',\n",
       "  'history',\n",
       "  ':',\n",
       "  'england',\n",
       "  'bundled',\n",
       "  'australia',\n",
       "  '60',\n",
       "  '...',\n",
       "  'http://t.co/t5trhjuau0'],\n",
       " ['hail',\n",
       "  'carol',\n",
       "  'stream',\n",
       "  ',',\n",
       "  'illinois',\n",
       "  'great',\n",
       "  'michigan',\n",
       "  'technique',\n",
       "  'camp',\n",
       "  'b1g',\n",
       "  'thanks',\n",
       "  '@bmurph1019',\n",
       "  '@hail_youtsey',\n",
       "  '.',\n",
       "  '@termn8r13',\n",
       "  '#goblue',\n",
       "  '#wrestleon',\n",
       "  'http://t.co/oaskgki6qj']]"
      ]
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[custom_tokenizer_lemma(t) for t in train['text'][:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ячейку ниже лучше не запускать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec_lemma = CountVectorizer(tokenizer = custom_tokenizer_lemma)\n",
    "X_train = cnt_vec_lemma.fit_transform(train['text'])\n",
    "X_test = cnt_vec_lemma.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7409410492157923\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82      1318\n",
      "           1       0.78      0.71      0.74       966\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.78      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlanamaslennikova/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(f1_score(y_test, pred))\n",
    "print()\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "эх лемматизация не помогла, результат +- такой же, а компьютер нагрелся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer_2(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tw = TweetTokenizer()\n",
    "    text_preprocessed = [stemmer.stem(w) for w in tw.tokenize(text.lower()) if w not in stop_words]\n",
    "     \n",
    "    \n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec_lemma = CountVectorizer(tokenizer = custom_tokenizer_2)\n",
    "X_train = cnt_vec_lemma.fit_transform(train['text'])\n",
    "X_test = cnt_vec_lemma.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7571351642434032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83      1318\n",
      "           1       0.79      0.73      0.76       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.80      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlanamaslennikova/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(f1_score(y_test, pred))\n",
    "print()\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "удивительно, самый большой результат мы получили отказавшись от фичей с хэштэгами и проверкой на латиницу"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
